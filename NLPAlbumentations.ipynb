{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPAlbumentations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcA3M8Qm9KpY0B3SdXaggp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aflah02/NLP-Albumentations-Data-Augmentation/blob/main/NLPAlbumentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5vu_C_wEdjr",
        "outputId": "618f9b5a-9da4-4549-dd88-6c2ecf68ffb2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o5xTq5-PDAID"
      },
      "outputs": [],
      "source": [
        "def shuffleSentenceTransform(paragraph):\n",
        "  Sentences = list(nltk.tokenize.sent_tokenize(paragraph))\n",
        "  random.shuffle(Sentences)\n",
        "  return \" \".join(Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remDuplicateSentences(paragraph):\n",
        "  Sentences = list(nltk.tokenize.sent_tokenize(paragraph))\n",
        "  unique = set()\n",
        "  Sentences = [unique.add(n) or n for n in Sentences if n not in unique]\n",
        "  return \" \".join(Sentences)"
      ],
      "metadata": {
        "id": "nR7zjH3-FYpm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shuffleSentenceTransform(\"<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTcrgftAE80P",
        "outputId": "828f1d74-bf18-44f2-c4fb-691076d4cfe1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Sentence4>. <Sentence4>. <Sentence5>. <Sentence2>. <Sentence5>. <Sentence1>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(remDuplicateSentences(\"<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_xA1BdoFzok",
        "outputId": "020c2f3c-ce2d-4301-ade0-8b1274e89dae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Sentence1>. <Sentence2>. <Sentence4>. <Sentence5>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Download"
      ],
      "metadata": {
        "id": "-EZzi_DZDiYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\n",
        "!wget -q https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/dev.tsv"
      ],
      "metadata": {
        "id": "EgbaBVRADh1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Building Function"
      ],
      "metadata": {
        "id": "3ooBO3cFDk8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buildTrainTestDatasets(doAug, NumberOfTrainingSamplesTouse):\n",
        "  df_train = pd.read_csv('train.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "  df_dev = pd.read_csv('dev.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "  df_test = pd.read_csv('test.tsv', delimiter = '\\t', names = ['Sentence', 'Label'])\n",
        "\n",
        "  df_train = pd.concat([df_train, df_dev])\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_train = df_train.loc[:, [\"Label\", \"Sentence\"]]\n",
        "  df_test = df_test.loc[:, [\"Label\", \"Sentence\"]]\n",
        "  if (NumberOfTrainingSamplesTouse > len(df_train)):\n",
        "    raise Exception(\"More Samples Asked For Than Present\")\n",
        "\n",
        "  df_train = df_train.iloc[:NumberOfTrainingSamplesTouse]"
      ],
      "metadata": {
        "id": "WNEXXHDqDdbU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}