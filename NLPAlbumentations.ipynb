{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPAlbumentations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgo6vgjkgT53COpMdJINIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aflah02/NLP-Albumentations-Data-Augmentation/blob/main/NLPAlbumentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import random\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5vu_C_wEdjr",
        "outputId": "e04bea22-a174-4a93-acd4-6f4ba5e417b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o5xTq5-PDAID"
      },
      "outputs": [],
      "source": [
        "def shuffleSentenceTransform(paragraph):\n",
        "  Sentences = list(nltk.tokenize.sent_tokenize(paragraph))\n",
        "  random.shuffle(Sentences)\n",
        "  return \" \".join(Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def removeDuplicateSentences(paragraph):\n",
        "  Sentences = list(nltk.tokenize.sent_tokenize(paragraph))\n",
        "  unique = set()\n",
        "  Sentences = [unique.add(n) or n for n in Sentences if n not in unique]\n",
        "  return \" \".join(Sentences)"
      ],
      "metadata": {
        "id": "nR7zjH3-FYpm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeNumbers(sentence):\n",
        "  \"\"\"\n",
        "  Reference: https://stackoverflow.com/a/817328/13858953\n",
        "  \"\"\"\n",
        "  sentence = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\",\"\",sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "TGbBLbMMGpls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeHashtags(sentence, removeWords):\n",
        "  tk = TweetTokenizer()\n",
        "  words = tk.tokenize(sentence)\n",
        "  if (removeWords):\n",
        "    noHashtagWords = []\n",
        "    for i in range(len(words)):\n",
        "      if words[i][0] == \"#\":\n",
        "        continue\n",
        "      noHashtagWords.append(words[i])\n",
        "    return \" \".join(noHashtagWords)                            \n",
        "  else:\n",
        "    noHashtags = []\n",
        "    for i in range(len(words)):\n",
        "      if words[i][0] == \"#\" and len(words[i]) > 1:\n",
        "        noHashtags.append(words[i][1:])\n",
        "        continue\n",
        "      noHashtags.append(words[i])\n",
        "    return \" \".join(noHashtags) "
      ],
      "metadata": {
        "id": "46tvcMwCN16m"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeUsers(sentence, keepNames):\n",
        "  tk = TweetTokenizer()\n",
        "  words = tk.tokenize(sentence)\n",
        "  if (not keepNames):\n",
        "    noUserNameWords = []\n",
        "    for i in range(len(words)):\n",
        "      if words[i][0] == \"@\":\n",
        "        continue\n",
        "      noUserNameWords.append(words[i])\n",
        "    return \" \".join(noUserNameWords)                            \n",
        "  else:\n",
        "    UserNameWords = []\n",
        "    for i in range(len(words)):\n",
        "      if words[i][0] == \"@\" and len(words[i]) > 1:\n",
        "        UserNameWords.append(words[i][1:])\n",
        "        continue\n",
        "      UserNameWords.append(words[i])\n",
        "    return \" \".join(UserNameWords) "
      ],
      "metadata": {
        "id": "lPHmij-uE7Hp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeURLs(sentence):\n",
        "  \"\"\"\n",
        "    regex taken from https://stackoverflow.com/a/28552670/13858953\n",
        "  \"\"\"\n",
        "  URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:\\'\\\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
        "  return re.sub(URL_REGEX, \"\", sentence)"
      ],
      "metadata": {
        "id": "wcPUWlkDFhX3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shuffleSentenceTransform(\"<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.\"))\n",
        "print(removeDuplicateSentences(\"<Sentence1>. <Sentence2>. <Sentence4>. <Sentence4>. <Sentence5>. <Sentence5>.\"))\n",
        "print(removeNumbers(\"This must not b3 delet3d, but the number at the end yes -134.411\"))\n",
        "print(removeHashtags(\"this tweet is example #key1_key2_key3\",False))\n",
        "print(removeHashtags(\"this tweet is example #key1_key2_key3\",True))\n",
        "print(removeUsers(\"this tweet is example @Aflah_Gamer\",False))\n",
        "print(removeUsers(\"this tweet is example @Aflah_Gamer\",True))\n",
        "print(removeURLs(\"this tweet is example @Aflah_Gamer http://foo.com/more_(than)_one_(parens)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTcrgftAE80P",
        "outputId": "5c4b4147-10f0-447d-f75a-18fc39904fb3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Sentence4>. <Sentence2>. <Sentence5>. <Sentence4>. <Sentence1>. <Sentence5>.\n",
            "<Sentence1>. <Sentence2>. <Sentence4>. <Sentence5>.\n",
            "This must not b3 delet3d, but the number at the end yes\n",
            "this tweet is example key1_key2_key3\n",
            "this tweet is example\n",
            "this tweet is example\n",
            "this tweet is example Aflah_Gamer\n",
            "this tweet is example @Aflah_Gamer \n"
          ]
        }
      ]
    }
  ]
}